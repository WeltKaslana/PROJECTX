from langchain import hub
from langsmith import Client
from langchain_deepseek import ChatDeepSeek
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.prompts import StringPromptTemplate, MessagesPlaceholder

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage

from langchain_core.output_parsers import PydanticOutputParser, XMLOutputParser
from langchain_core.output_parsers import StrOutputParser
from langchain.output_parsers import RetryOutputParser
from langchain_core.exceptions import OutputParserException

from langchain_core.runnables import RunnableParallel, chain, ConfigurableField
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, ConfigurableFieldSpec
from langchain_core.runnables.history import RunnableWithMessageHistory

from langchain_core.chat_history import BaseChatMessageHistory

from langchain_core.vectorstores import InMemoryVectorStore
from langchain_community.vectorstores import FAISS

from langchain_core.document_loaders import BaseLoader
from langchain_core.documents import Document

from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_community.document_loaders import TextLoader, UnstructuredExcelLoader
from langchain_community.document_loaders.csv_loader import CSVLoader

from langchain_text_splitters import CharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain.runnables.hub import HubRunnable
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_unstructured import UnstructuredLoader
from langchain_pinecone import PineconeVectorStore
from langchain_redis import RedisChatMessageHistory

import asyncio, base64, io, fitz, bs4, getpass, time, inspect, json, os
from PIL import Image
from dotenv import load_dotenv
from uuid import uuid4
from operator import itemgetter
from typing import Iterator, List, AsyncIterator
from pydantic import BaseModel, Field, model_validator
from IPython.display import Image as IPImage
from IPython.display import display
from pinecone import Pinecone, ServerlessSpec

load_dotenv()

class CustomDocumentLoader(BaseLoader):
    """é€è¡Œè¯»å–æ–‡ä»¶çš„æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹"""
    def __init__(self, file_path: str) -> None:
        """ä½¿ç”¨æ–‡ä»¶è·¯å¾„åˆå§‹åŒ–åŠ è½½å™¨
        å‚æ•°:file_path: è¦åŠ è½½çš„æ–‡ä»¶è·¯å¾„
        """
        self.file_path = file_path

    def lazy_load(self) -> Iterator[Document]:
        """é€è¡Œè¯»å–æ–‡ä»¶çš„æƒ°æ€§åŠ è½½å™¨
        å½“å®ç°æƒ°æ€§åŠ è½½æ–¹æ³•æ—¶,ä½ åº”è¯¥ä½¿ç”¨ç”Ÿæˆå™¨
        ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªæ–‡æ¡£
        """
        with open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

    # alazy_load æ˜¯å¯é€‰çš„
    # å¦‚æœä¸å®ç°å®ƒ,å°†ä½¿ç”¨ä¸€ä¸ªé»˜è®¤å®ç°,è¯¥å®ç°ä¼šå§”æ‰˜ç»™ lazy_load!
    async def alazy_load(
        self,
    ) -> AsyncIterator[Document]:  # <-- ä¸æ¥å—ä»»ä½•å‚æ•°
        """é€è¡Œè¯»å–æ–‡ä»¶çš„å¼‚æ­¥æƒ°æ€§åŠ è½½å™¨"""
        # éœ€è¦ aiofiles (é€šè¿‡ pip æˆ– uv å®‰è£…)
        # https://github.com/Tinche/aiofiles
        import aiofiles

        async with aiofiles.open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            async for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

def main():
    key = os.getenv("SILICONFLOW_API_KEY")
    base = os.getenv("SILICONFLOW_API_BASE")
    langsmith_key = os.getenv("LANGSMITH_API_KEY") #æ¨¡ç‰ˆç½‘ç«™çš„api_key
    model = ChatDeepSeek(
        model='deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',
        temperature=0,
        api_key=key,
        api_base=base,
    ).configurable_fields(
        temperature=ConfigurableField(
            id="temperature",
            name="æ¨¡å‹æ¸©åº¦",
            description="The temperature to use for the model."
        ),
    )

    #code1  å­—ç¬¦ä¸²æ¨¡ç‰ˆ
    def code1():
        prompt = PromptTemplate.from_template("ä½ æ˜¯â¼€ä¸ª{name}ï¼Œå¸®æˆ‘èµ·â¼€ä¸ªå…·æœ‰{country}ç‰¹â¾Šçš„{sex}åå­—")
        prompts = prompt.format(name="ç®—å‘½â¼¤å¸ˆ", country="ä¸­å›½", sex="â¼¥å­©")
        print(prompts)

    #code2 å¯¹è¯æ¨¡ç‰ˆ
    def code2():
        chat_template = ChatPromptTemplate.from_messages(
            [
                {"role": "system", "content": "ä½ æ˜¯â¼€ä¸ªèµ·åâ¼¤å¸ˆï¼Œä½ çš„åå­—å«{name}"},
                {"role": "human", "content": "ä½ å¥½{name}ï¼Œä½ æ„Ÿè§‰å¦‚ä½•ï¼Ÿ "},
                {"role": "ai", "content": "ä½ å¥½ï¼æˆ‘çŠ¶æ€â¾®å¸¸å¥½ï¼ "},
                {"role": "human", "content": "ä½ å«ä»€ä¹ˆåå­—å‘¢ï¼Ÿ "},
                {"role": "ai", "content": "ä½ å¥½ï¼æˆ‘å«{name}"},
                {"role": "human", "content": "{user_input}"},
            ]
        )
        chats = chat_template.format_messages(name="å•åŠä»™â¼‰", user_input="ä½ çš„çˆ·çˆ·æ˜¯è°å‘¢ï¼Ÿ ")
        print(chats)

    #code3  å ä½ç¬¦æ¨¡ç‰ˆ
    def code3():
        prompt_template = ChatPromptTemplate(
            [
                {"role": "system", "content": "ä½ æ˜¯â¼€ä¸ªè¶…çº§â¼ˆâ¼¯æ™ºèƒ½åŠ©â¼¿"},
                #MessagesPlaceholder("msgs")
                {"role":"placeholder","content" :"{msgs}"}
            ]
        )
        result = prompt_template.invoke({"msgs":[HumanMessage("è¯·å¸®æˆ‘å†™â¼€ä¸ªå…³äºæœºå™¨å­¦ä¹ çš„â½‚ç« ")]})
        print(result)

    #code4 ä½¿ç”¨Messageç»„åˆæ¨¡ç‰ˆ
    def code4():
        sys = SystemMessage(
            content="ä½ æ˜¯â¼€ä¸ªèµ·åâ¼¤å¸ˆ",
            additional_kwargs = {"â¼¤å¸ˆå§“å": "å•åŠä»™â¼‰"}
        )
        human = HumanMessage(
            content="è¯·é—®â¼¤å¸ˆå«ä»€ä¹ˆåå­—ï¼Ÿ "
        )
        ai = AIMessage(
            content="æˆ‘å«å•åŠä»™â¼‰"
        )
        mesg = [sys,human,ai]
        print(mesg)

    #code5 è‡ªå®šä¹‰æ¨¡ç‰ˆ
    def code5():
        def hello_world(abc):
            print("Hello World!")
            return abc
        
        PROMPT = """\
        ä½ æ˜¯â¼€ä¸ªâ¾®å¸¸æœ‰ç»éªŒå’Œå¤©èµ‹çš„ç¨‹åºå‘˜ï¼Œç°åœ¨ç»™ä½ å¦‚ä¸‹å‡½æ•°åç§°ï¼Œä½ ä¼šæŒ‰ç…§å¦‚ä¸‹æ ¼å¼ï¼Œè¾“å‡ºè¿™æ®µä»£ç çš„åç§°ã€æºä»£ç ã€ä¸­â½‚è§£é‡Šã€‚
        å‡½æ•°åç§°ï¼š {function_name}
        å‡½æ•°æºä»£ç :\n{source_code}
        ä»£ç è§£é‡Šï¼š
        """
        def get_source_code(function_name):
            #è·å¾—æºä»£ç 
            return inspect.getsource(function_name)
        #â¾ƒå®šä¹‰æ¨¡æ¿class
        class CustomPromptTemplate(StringPromptTemplate):
            def format(self, **kwargs) -> str:
                # è·å–æºä»£ç 
                source_code = get_source_code(kwargs["function_name"])
                #â½£æˆæç¤ºè¯æ¨¡æ¿
                prompt = PROMPT.format(
                    function_name=kwargs["function_name"].__name__,
                    source_code=source_code,
                )
                return prompt
        #ä½¿â½¤â¾ƒå®šä¹‰çš„æç¤ºè¯æ¨¡æ¿ï¼Œâ½½å¹¶â¾®ç±»ä¼¼åº¦åŒ–æç¤ºè¯æ¨¡æ¿
        a = CustomPromptTemplate(input_variables=["function_name"])
        pm = a.format(function_name= hello_world)
        print("æ ¼å¼åŒ–ä¹‹åçš„æç¤ºè¯ä¸º=======>")
        print(pm)

    #code6 pushã€pullè‡ªå®šä¹‰æ¨¡ç‰ˆæ¨¡ç‰ˆ
    def code6():
        client=Client(api_key=langsmith_key)
        # ä½¿â½¤ from_messages æ„é€ å¤šè½®å¯¹è¯ç»“æ„
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "You are a super intelligent assistant"),
                ("human", "{question}"),
            ]
        )
        project_name = "test1_langsmith"
        # æ¨é€æç¤ºè¯åˆ° LangSmithï¼Œå¹¶æŒ‡å®šæ‰€å±é¡¹â½¬
        try: #å¦‚æœæœªæ›´æ–°æ¨¡ç‰ˆå°±pushä¼šæŠ¥é”™
            client.push_prompt(project_name, object=prompt)
        except Exception as e:
            print(e)
        # pro = client.pull_prompt(project_name)
        # print(pro.invoke({"question":"What is the capital of France?"}))
        # res = model.invoke(pro.invoke({"question":"What is the capital of France?"}))
        # print(res)

    #code7 è°ƒç”¨å…¬å…±çš„hubæ¨¡ç‰ˆ
    def code7():
        client=Client(api_key=langsmith_key)
        pro=client.pull_prompt("rlm/rag-prompt")
        # print(pro)
        res = model.invoke(pro.invoke({
            "context": "China is a great country. It is a country of peace and love.",
            "question":"Who is the current leader of China?"}))
        print(res)
    
    #code8 æ¨¡å‹è¾“å‡ºè§£æã€é‡è¯•
    def code8():
        class Joke(BaseModel):
            setup: str = Field(..., description="ç¬‘è¯ä¸­çš„é“ºå«é—®é¢˜ï¼Œå¿…é¡»ä»¥ï¼Ÿç»“å°¾")
            punchline: str = Field(..., description="ç¬‘è¯ä¸­å›ç­”é“ºå«é—®é¢˜çš„éƒ¨åˆ†ï¼Œé€šå¸¸æ˜¯â¼€ç§æŠ–åŒ…è¢±â½…å¼å›ç­”é“ºå«é—®é¢˜ï¼Œä¾‹å¦‚è°â¾³ã€ä¼šé”™æ„ç­‰")
            # # å¯ä»¥æ ¹æ®â¾ƒâ¼°çš„æ•°æ®æƒ…å†µè¿›â¾â¾ƒå®šä¹‰éªŒè¯å™¨
            @model_validator(mode="before")
            @classmethod
            def check_setup_and_punchline(cls, values:dict) -> dict:
                setup = values.get("setup")
                punchline = values.get("punchline")
                if setup and punchline and (setup.endswith("? ") or setup.endswith("ï¼Ÿ")):
                    return values
                else:
                    raise ValueError("setup and punchline must be provided and setup must end with'?'")
        parser = PydanticOutputParser(pydantic_object=Joke)

        # parser = XMLOutputParser()
        # parser = XMLOutputParser(tags = ["setup", "punchline"])

        # æ³¨æ„ï¼Œæç¤ºè¯æ¨¡æ¿ä¸­éœ€è¦éƒ¨åˆ†æ ¼å¼åŒ–è§£æå™¨çš„æ ¼å¼è¦æ±‚format_instructions
        prompt = PromptTemplate(
            template="å›ç­”â½¤æˆ·çš„æŸ¥è¯¢.\n{format_instructions}\n{query}",
            input_variables=["query"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        # print("pydanticoutparserçš„æ ¼å¼è¦æ±‚ä¸º=======>")
        # print(parser.get_format_instructions())
        chain = prompt | model 
        prompt_value=prompt.format_prompt(query="è¯·å†™ä¸€ä¸ªç¬‘è¯")
        output = chain.invoke(prompt_value)
        print(type(output))
        res = output.content
        res = {
            "setup": "hello",
            "punchline": "world",
        }

        res=json.dumps(res)
        print(type(res))

        try:
            parser.parse(res)
            print(res)
        except Exception as e:
            # print(e)
            retry = RetryOutputParser.from_llm(parser=parser, llm=model)
            retry_res = retry.parse_with_prompt(res, prompt_value)
            print(retry_res)
    
    # code9 å¹¶è¡Œchainè¿è¡Œå¤§æ¨¡å‹
    def code9():
        joke_prompt =ChatPromptTemplate.from_template("tell me a joke about {topic1}")
        story_prompt = ChatPromptTemplate.from_template("tell me a short story about {topic2}")
        # joke_chain = joke_prompt | model | StrOutputParser()
        joke_chain = joke_prompt.pipe(model).pipe(StrOutputParser())
        story_chain = story_prompt | model | StrOutputParser()
        
        map_chain = RunnableParallel(joke = joke_chain, topic = story_chain)
        print(map_chain.invoke({"topic1": "football","topic2": "programming"}))

    #code10 åˆ©ç”¨@chainä¿®é¥°ç¬¦å°†å‡½æ•°è½¬ä¸ºchain
    def code10():
        prompt1 = ChatPromptTemplate.from_template("tell me a joke about {topic}")
        prompt2 = ChatPromptTemplate.from_template("tell me the subject of this joke: {joke}")

        @chain
        def defchain(text):
            joke_prompt = prompt1.invoke({"topic":text})
            joke = model.invoke(joke_prompt)
            output1 = StrOutputParser().parse(joke)
            print(output1)

            chain2 = prompt2 | model | StrOutputParser()
            print(chain2.invoke({"joke":output1}))
        defchain.invoke("football")
    
    #code 11 lambaå‡½æ•°\streamè¾“å‡º\chainä¸­è‡ªå®šä¹‰å‡½æ•°
    def code11():
        def length_function(text):
            return len(text)
        def _multiple_length_function(text1,text2):
            return len(text1) * len(text2)
        def multiple_length_function(_dict):
            return _multiple_length_function(_dict["text1"],_dict["text2"])
        prompt = ChatPromptTemplate.from_template("What is the sum of {a} and {b}?")
        chain =(
            {"a": itemgetter("foo") | RunnableLambda(length_function),
            "b": {"text1": itemgetter("foo"), "text2": itemgetter("bar")}
            | RunnableLambda(multiple_length_function)
            }
            | prompt | model | StrOutputParser()
        )
        def func(input: Iterator) -> Iterator[List]:
            buffer = ""
            for chunk in input:
                buffer += chunk
                while "," in buffer:
                    comma_index = buffer.index(",")
                    yield [buffer[:comma_index].strip()]
                    buffer = buffer[comma_index + 1:]
            yield [buffer.strip()]

        chain = chain | func
        for i in chain.stream({"foo": "hello", "bar": "world"}):
            print(i, end="", flush=True)
    
    #code 12 runnablePassThrough
    def code12():
        runnable = RunnableParallel(
            passed = RunnablePassthrough(),
            moddified = lambda x: x["num"] + 2,
        )
        res = runnable.invoke({"num": 1})
        print(res)
    
    #code 13 åŠ¨æ€åŠ è½½æ¨¡å‹é…ç½®ã€æç¤ºè¯
    def code13():
        prompt = HubRunnable("rlm/rag-prompt").configurable_fields(
            owner_repo_commit=ConfigurableField(
                id="hub_commit",
                name="Hub Commit",
                description="The Hub commit to pull from."
            ),
        )
        prompt = prompt.with_config(configurable = {"hub_commit":"rlm/rag-prompt"})
        prompt = prompt.invoke({"question":"foo","context":"bar"})
        res = model.invoke(prompt, config={"temperature":1})
        res = StrOutputParser().parse(res)
        print(res)
    
    #code 14 å†…å­˜å†…è®°å¿†ã€çŸ­æœŸè®°å¿†
    def code14():
        class InMemoryChatMessageHistory(BaseChatMessageHistory, BaseModel):
            messages: List[BaseMessage] = Field(default_factory=list)
            def add_messages(self, messages: List[BaseMessage]):
                self.messages.extend(messages)
            def clear(self):
                self.messages = []
        stored_history={}
        # ç´¢å¼•ä¸ºç”¨æˆ·
        def get_history(session_id: str) -> BaseChatMessageHistory:
            if session_id not in stored_history:
                stored_history[session_id] = InMemoryChatMessageHistory()
            return stored_history[session_id]
        #ç´¢å¼•ä¸ºç”¨æˆ·+å¯¹è¯ID
        def get_session(user_id: str, conversation_id: str)->BaseChatMessageHistory:
            if (user_id, conversation_id) not in stored_history:
                stored_history[(user_id, conversation_id)] = InMemoryChatMessageHistory()
            return stored_history[(user_id, conversation_id)]
        history = get_history("1")
        history.add_messages([AIMessage(content="Hello! æˆ‘æ˜¯Deepseek")])
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿{ability}çš„åŠ©æ‰‹"), # ç³»ç»Ÿè§’è‰²æç¤º,ä½¿ç”¨abilityå˜é‡å®šä¹‰åŠ©æ‰‹ä¸“é•¿
            MessagesPlaceholder(variable_name="history"), # æ”¾ç½®å†å²æ¶ˆæ¯çš„å ä½ç¬¦
            ("human", "{question}"), # ç”¨æˆ·é—®é¢˜çš„å ä½ç¬¦
        ])
        chain = prompt | model | StrOutputParser()
        # åˆ›å»ºå¸¦å†å²æ¶ˆæ¯çš„é“¾ï¼Œç´¢å¼•ä¸ºç”¨æˆ·id
        # chain_with_history = RunnableWithMessageHistory(
        #     chain,
        #     get_session_history=get_history,
        #     input_messages_key="question", # æŒ‡å®šè¾“å…¥æ¶ˆæ¯çš„é”®å
        #     history_messages_key="history", # æŒ‡å®šå†å²æ¶ˆæ¯çš„é”®å
        # )
        # print(chain_with_history.invoke(
        #     {"ability": "å†™ç¬‘è¯", "question": "è¯·å†™ä¸€ä¸ªç¬‘è¯"},
        #     config={"configurable": {"session_id": "1"}}
        # ))
        # print(chain_with_history.invoke(
        #     {"ability": "å†™ç¬‘è¯", "question": "ä¸Šé¢ç¬‘è¯çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"},
        #     config={"configurable": {"session_id": "1"}}
        # ))
        
        #åˆ›å»ºä¸€ä¸ªå¸¦å†å²æ¶ˆæ¯çš„é“¾ï¼Œç´¢å¼•ä¸ºç”¨æˆ·id+ä¼šè¯id
        with_message_history = RunnableWithMessageHistory(
            chain,
            get_session_history=get_session,
            input_messages_key="question", # è¾“å…¥æ¶ˆæ¯çš„é”®å
            history_messages_key="history", # å†å²æ¶ˆæ¯çš„é”®å
            history_factory_config=[ # å†å²è®°å½•å·¥å‚é…ç½®
                ConfigurableFieldSpec(
                    id="user_id", # é…ç½®å­—æ®µID
                    annotation=str, # ç±»å‹æ³¨è§£
                    name="ç”¨æˆ·ID", # å­—æ®µåç§°
                    description="ç”¨æˆ·çš„å”¯ä¸€æ ‡è¯†ç¬¦", # å­—æ®µæè¿°
                    default="", # é»˜è®¤å€¼
                    is_shared=True, # æ˜¯å¦åœ¨å¤šä¸ªè°ƒç”¨é—´å…±äº«
                ),
                ConfigurableFieldSpec(
                    id="conversation_id", # é…ç½®å­—æ®µID
                    annotation=str, # ç±»å‹æ³¨è§£
                    name="å¯¹è¯ID", # å­—æ®µåç§°
                    description="å¯¹è¯çš„å”¯ä¸€æ ‡è¯†ç¬¦", # å­—æ®µæè¿°
                    default="", # é»˜è®¤å€¼
                    is_shared=True, # æ˜¯å¦åœ¨å¤šä¸ªè°ƒç”¨é—´å…±äº«
                ),
            ],
        )
        with_message_history.invoke(
            {"ability": "å†™ç¬‘è¯", "question": "è¯·å†™ä¸€ä¸ªç¬‘è¯"},
            config={"configurable": {"user_id": "1", "conversation_id": "1"}}
        )
        with_message_history.invoke(
            {"ability": "å†™ç¬‘è¯", "question": "ä¸Šé¢ç¬‘è¯çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"},
            config={"configurable": {"user_id": "1", "conversation_id": "1"}}
        )
        print(stored_history)
    
    #code 15 é•¿æœŸè®°å¿†
    def code15():
        def get_history(session_id: str) -> RedisChatMessageHistory:
            return RedisChatMessageHistory(
                session_id=session_id,
                redis_url="redis://47.98.143.59:6379",
            )
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿{ability}çš„åŠ©æ‰‹"), # ç³»ç»Ÿè§’è‰²æç¤º,ä½¿ç”¨abilityå˜é‡å®šä¹‰åŠ©æ‰‹ä¸“é•¿
            MessagesPlaceholder(variable_name="history"), # æ”¾ç½®å†å²æ¶ˆæ¯çš„å ä½ç¬¦
            ("human", "{question}"), # ç”¨æˆ·é—®é¢˜çš„å ä½ç¬¦
        ])
        chain = prompt | model
        with_history = RunnableWithMessageHistory(
            chain,
            get_session_history=get_history,
            input_messages_key="question", # è¾“å…¥æ¶ˆæ¯çš„é”®å
            history_messages_key="history", # å†å²æ¶ˆæ¯çš„é”®å
        )
        history = get_history("uesr1")
        history.clear()
        with_history.invoke(
            { "ability":"å†™ç¬‘è¯", "question": "è¯·å†™ä¸€ä¸ªç¬‘è¯"},
            config={"configurable": {"session_id": "uesr1"}}
        )
        with_history.invoke(
            { "ability":"å†™ç¬‘è¯","question": "ä¸Šé¢ç¬‘è¯çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"},
            config={"configurable": {"session_id": "uesr1"}}
        )
        print("èŠå¤©å†å²ï¼š ")
        for message in history.messages:
            # æ‰“å°æ¯æ¡æ¶ˆæ¯çš„ç±»å‹å’Œå†…å®¹
            print(f"{type(message).__name__}: {message.content}")
    # code15()
    
    #code 16 è‡ªå®šä¹‰è·¯ç”±é“¾
    def code16 ():
        chain = (
            PromptTemplate.from_template(
                """æ ¹æ®ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜ï¼Œ å°†å…¶åˆ†ç±»ä¸º `LangChain`ã€ `Anthropic` æˆ– `Other`ã€‚
                è¯·åªå›å¤ä¸€ä¸ªè¯ä½œä¸ºç­”æ¡ˆã€‚
                <question>
                {question}
                </question>
                åˆ†ç±»ç»“æœ:"""
                ) | model | StrOutputParser() 
        )
        langchain_chain = PromptTemplate.from_template(
            """ä½ å°†æ‰®æ¼”ä¸€ä½LangChainä¸“å®¶ã€‚ è¯·ä»¥ä»–çš„è§†è§’å›ç­”é—®é¢˜ã€‚ \
            ä½ çš„å›ç­”å¿…é¡»ä»¥"æ­£å¦‚Harrison Chaseå‘Šè¯‰æˆ‘çš„"å¼€å¤´ï¼Œ å¦åˆ™ä½ ä¼šå—åˆ°æƒ©ç½šã€‚ \
            è¯·å›ç­”ä»¥ä¸‹é—®é¢˜:
            é—®é¢˜: {question}
            å›ç­”:"""
            ) | model
        anthropic_chain = PromptTemplate.from_template(
            """ä½ å°†æ‰®æ¼”ä¸€ä½ä¸€ä½Anthropicä¸“å®¶ã€‚ è¯·ä»¥ä»–çš„è§†è§’å›ç­”é—®é¢˜ã€‚ \
            ä½ çš„å›ç­”å¿…é¡»ä»¥"æ­£å¦‚Dario Amodeiå‘Šè¯‰æˆ‘çš„"å¼€å¤´ï¼Œ å¦åˆ™ä½ ä¼šå—åˆ°æƒ©ç½šã€‚ \
            è¯·å›ç­”ä»¥ä¸‹é—®é¢˜:
            é—®é¢˜: {question}
            å›ç­”:"""
            ) | model
        general_chain = PromptTemplate.from_template(
            """è¯·å›ç­”ä»¥ä¸‹é—®é¢˜:
            é—®é¢˜: {question}
            å›ç­”:"""
            ) | model
        
        def route(info):
            print(info)
            if "anthropic" in info["topic"].lower():
                print("claude")
                return anthropic_chain
            elif "langchain" in info["topic"].lower():
                print("langchain")
                return langchain_chain
            else:
                print("general")
                return general_chain
            
        fall_chain={
            "topic":chain, "question": lambda x:x["question"]
        } | RunnableLambda(route) | StrOutputParser()

        res = fall_chain.invoke({"question":"æˆ‘è¯¥å¦‚ä½•ä½¿ç”¨langchain?"})
        print(res)
    
    #å¼‚æ­¥åŠ è½½pdf
    async def load_pdf():
        file_path = "1.pdf"
        loader=PyPDFLoader(file_path)
        pages = []
        async for page in loader.alazy_load():
            pages.append(page)
        
        for page in pages:
            print(page.metadata)
            print(page.page_content)

    #è¯»å–pdfä¸­å›¾ç‰‡
    def pdf_page_to_base64(page_number: int):
        pdf_document= fitz.open("E:\\1.pdf")
        page = pdf_document.load_page(page_number - 1) # input is one-indexed
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        buffer = io.BytesIO()
        img.save(buffer, format="PNG")
        return base64.b64encode(buffer.getvalue()).decode("utf-8")
    
    #å¤šæ¨¡æ€
    def code17():
        query = "What is the subject of picture?"
        message = HumanMessage(
            content=[
                {"type": "text", "text": query},
                {
                    "type": "image_url",
                    "image _url": {"url": f"data: image/jpeg;base64,{pdf_page_to_base64(1)}"}
                }
            ]
        )
        model = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        print(1)
        res = model.invoke([message])
        print(res)
    
    page_url = "https://python.langchain.com/docs/how_to/chatbots_memory/"
    #åŠ è½½ç½‘å€
    async def loadweb(page_url):
        loader = WebBaseLoader(web_paths=[page_url])
        docs =[]
        async for doc in loader.alazy_load():
            docs.append(doc)
        assert len(docs) == 1
        doc = docs[0]
        print(f"{doc.metadata}\n")
        print(doc.page_content[:500].strip())
    # asyncio.run(loadweb(page_url))
    
    #åŠ è½½éƒ¨åˆ†ç½‘é¡µ
    async def loadwebPart(page_url,page_num):
        loader = WebBaseLoader(
            web_paths=[page_url],
            bs_kwargs={
                "parse_only": bs4.SoupStrainer(class_="theme-doc-markdow markdown"),
            },
            bs_get_text_kwargs={"separator": " | ", "strip": True},
        )
        docs=[]
        async for doc in loader.alazy_load():
            docs.append(doc)
        assert len(docs) == 1
        doc = docs[page_num]
        print(f"{doc.metadata}\n")
        print(doc.page_content[:500])
    # asyncio.run(loadwebPart(page_url,0))

    #ä¸ç†Ÿæ‚‰ç½‘é¡µç»“æ„æ—¶è§£æç½‘é¡µ
    def loadweb_parse(page_url):
        try:
            loader = UnstructuredLoader(web_url=page_url)
            print(0)
            docs = list(loader.load())
            print(1)
            for doc in docs[:5]:
                print(f"{doc.metadata}\n")
                print(doc.page_content[:500])
        except Exception as e:
            print(e)
    # loadweb_parse("https://example.com")

    csv_path = "lin_try/hellochain/info.csv"
    #åŠ è½½csvæ–‡ä»¶
    def loadcsv(file_path):
        loader = CSVLoader(file_path=file_path, encoding="utf-8")
        data = loader.load()
        for record in data[:2]:
            print(record)

        # æŒ‡å®šâ¼€åˆ—æ¥æ ‡è¯†â½‚æ¡£
        loader = CSVLoader(file_path=file_path, source_column="é‚®ç®±", encoding="utf-8")
        data = loader.load()
        for record in data[:2]:
            print(record)
    # loadcsv(csv_path)

    xlsx_path = "lin_try/hellochain/scores.xlsx"
    # åŠ è½½excelæ–‡ä»¶
    def loadexcel(xlsx_path):
        # åŠ è½½æ‰€æœ‰å·¥ä½œè¡¨
        loader = UnstructuredExcelLoader(
            xlsx_path,
            mode="elements",
            process_multiple_sheets=True
        )
        documents = loader.load()
        print(f"Loaded {len(documents)} documents from {xlsx_path}")
        for doc in documents:
            print(f"Metadata: {doc.metadata}")
            print(f"Content: {doc.page_content[:500]}...")  # Print first 500 characters of content
            print("-" * 80)
    # loadexcel(xlsx_path)

    def loadcustom():
        with open("lin_try/hellochain/meow.txt", "w", encoding="utf-8") as f:
            quality_content = "meow meowğŸ± \n meow meowğŸ± \n meowğŸ˜»ğŸ˜»"
            f.write(quality_content)

        loader = CustomDocumentLoader("lin_try/hellochain/meow.txt")

        ## æµ‹è¯•æ‡’åŠ è½½
        for doc in loader.lazy_load():
            print()
            print(type(doc))
            print(doc)
    # loadcustom()
    
    def split():
        file_path = "lin_try/hellochain/deepseek.pdf"
        loader = PyPDFLoader(file_path)
        pages =[]
        for page in loader.load():
            pages.append(page)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)
        texts = text_splitter.split_text(pages[1].page_content)
        print(texts)
        print("-"*50)

        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
            encoding_name="cl100k_base", chunk_size=50, chunk_overlap=10
        )
        texts = text_splitter.split_text(pages[1].page_content)
        print(texts)
        print( "= " * 50)
        docs = text_splitter.create_documents([pages[2].page_content,pages[3].page_content])
        print(docs)
    # split()

    #FAISSç¼“å­˜
    def embed():
        embeddings_model = OpenAIEmbeddings(
            openai_api_key=key, 
            openai_api_base=base, 
            model="BAAI/bge-m3"
        )
        embeddings = embeddings_model.embed_documents(
            [
                "Hi there!",
                "Oh, hello!",
                "What's your name?",
                "My friends call me World",
                "Hello World!"
            ]
        )
        # embed_query
        query_embedding = embeddings_model.embed_query("What is the meaning of life?")
        # print(query_embedding)
        store = LocalFileStore("lin_try/hellochain/cache")
        cached_embedder = CacheBackedEmbeddings.from_bytes_store(
            embeddings_model, store, namespace=embeddings_model.model
        )
        print(list(store.yield_keys()))
        print("-"*50)
        raw_documents = TextLoader("lin_try/hellochain/meow.txt").load()
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        documents = text_splitter.split_documents(raw_documents)
        # åˆ›å»ºå‘é‡å­˜å‚¨
        db = FAISS.from_documents(documents, cached_embedder)

        # å†æ¬¡åˆ›å»ºå°†ä¼šè¯»å–ç¼“å­˜ï¼Œä»è€ŒåŠ å¿«é€Ÿåº¦é™ä½æˆæœ¬
        db2 = FAISS.from_documents(documents, cached_embedder)
        # æŸ¥çœ‹ç¼“å­˜
        print(list(store.yield_keys())[:5])
    # embed()

    # å°†vectorå­˜å‚¨åˆ°å†…å­˜ä¸­
    def vector():
        embeddings_model = OpenAIEmbeddings(
            openai_api_key = key, 
            openai_api_base = base, 
            model="BAAI/bge-m3"
        )
        vector_store = InMemoryVectorStore(embedding=embeddings_model)
        document_1 = Document(
            page_content="ä»Šå¤©åœ¨æŠ–éŸ³å­¦ä¼šäº†ä¸€ä¸ªæ–°èœï¼šé”…å·´åœŸè±†æ³¥ï¼çœ‹èµ·æ¥ç®€å•ï¼Œå®é™…ç‚¸äº†å¨æˆ¿ï¼Œè¿çŒ«éƒ½å«Œå¼ƒåœ°èµ°å¼€äº†ã€‚",
            metadata={"source": "ç¤¾äº¤åª’ä½“"},
        )
        document_2 = Document(
            page_content="å°åŒºé›ç‹—å¤§çˆ·ä»Šæ—¥æ’­æŠ¥ï¼šå¹¿åœºèˆå¤§å¦ˆå é¢†å¥èº«åŒºï¼Œé›ç‹—ç¾¤ä¼—çº·çº·æ’¤é€€ã€‚ç°åœºæ°”æ°›è¯¡å¼‚ï¼ŒBGMå·²å¾ªç¯æ’­æ”¾ã€Šæœ€ç‚«æ°‘æ—é£ã€‹ä¸¤å°æ—¶ã€‚",
            metadata={"source": "ç¤¾åŒºæ–°é—»"},
        )
        documents=[document_1, document_2]
        result = vector_store.add_documents(documents=documents, ids=["doc1", "doc2"])
        # idsä¸ºå¯é€‰é¡¹
        print(f"add {result}")
        print("-"*50)
        query = "é›ç‹—"
        # ç›´æ¥ç›¸ä¼¼æ€§æŸ¥è¯¢
        docs = vector_store.similarity_search(query=query)
        print(docs[0].page_content)
        print("-"*50)
        # ä½¿ç”¨å‘é‡æ¨¡å‹å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡ï¼Œå†åˆ©ç”¨å‘é‡è¿›è¡Œç›¸ä¼¼æ€§æŸ¥è¯¢
        embeddings_vector = embeddings_model.embed_query(query)
        docs = vector_store.similarity_search_by_vector(embeddings_vector)
        print(docs[0].page_content)
        print("-"*50)

        # åˆ é™¤å‘é‡
        vector_store.delete(ids=["doc2"])
        docs = vector_store.similarity_search_by_vector(embeddings_vector)
        print(docs[0].page_content)
        print("-"*50)
    # vector()

    # åˆ©ç”¨pineconeå­˜å‚¨å‘é‡
    def vector2():
        pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"),)
        index_name="test-index"
        existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]
        if index_name not in existing_indexes:
            # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            pc.create_index(
                name=index_name,
                dimension=4096, #æ³¨æ„ç»´åº¦è¦ä¸€è‡´
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"),
            )
            while not pc.describe_index(index_name).status["ready"]:
                time.sleep(1)
        # æŸ¥è¯¢pineconeæ•°æ®ä¿¡æ¯
        index_info = pc.describe_index(index_name)
        print(f"dimension: {index_info}")
        index = pc.Index(index_name)
        print(f"index = {index}")

        embeddings_model = OpenAIEmbeddings(
            openai_api_key = key, 
            openai_api_base = base, 
            model="Qwen/Qwen3-Embedding-8B"
        )
        vector_store = PineconeVectorStore(index=index, embedding=embeddings_model)
        # æµ‹è¯•æ•°æ®
        document_1 = Document(
            page_content="ä»Šå¤©æ—©é¤åƒäº†è€ç‹å®¶çš„ç”Ÿç…åŒ…ï¼Œé¦…æ–™å®åœ¨å¾—å¿«ä»è¤¶å­é‡Œè·³å‡ºæ¥äº†ï¼è¿™æ‰æ˜¯çœŸæ­£çš„ä¸Šæµ·å‘³é“ï¼",
            metadata={"source": "tweet"},
        )
        document_2 = Document(
            page_content="æ˜æ—¥å¤©æ°”é¢„æŠ¥ï¼šåŒ—äº¬åœ°åŒºå°†å‡ºç°å¤§èŒƒå›´é›¾éœ¾ï¼Œå»ºè®®å¸‚æ°‘æˆ´å¥½å£ç½©ï¼Œçœ‹ä¸è§è„¸çš„æ—¶å€™è¯·ä¸è¦æ…Œå¼ ã€‚",
            metadata={"source": "news"},
        )
        document_3 = Document(
            page_content="ç»ˆäºæå®šäº†AIèŠå¤©æœºå™¨äººï¼æˆ‘é—®å®ƒ'ä½ æ˜¯è°'ï¼Œå®ƒå›ç­”'æˆ‘æ˜¯ä½ çˆ¸çˆ¸'ï¼Œçœ‹æ¥è¿˜éœ€è¦è°ƒæ•™...",
            metadata={"source": "tweet"},
        )
        document_4 = Document(
            page_content="éœ‡æƒŠï¼æœ¬å¸‚ä¸€ç”·å­åœ¨ä¾¿åˆ©åº—æŠ¢åŠ«ï¼Œåªå› åº—å‘˜è¯´'æ‰«ç æ”¯ä»˜æ‰æœ‰ä¼˜æƒ 'ï¼Œç°å·²è¢«è­¦æ–¹æŠ“è·ã€‚",
            metadata={"source": "news"},
        )
        document_5 = Document(
            page_content="åˆšçœ‹å®Œã€Šæµæµªåœ°çƒ3ã€‹ï¼Œç‰¹æ•ˆç®€ç›´ç‚¸è£‚ï¼å°±æ˜¯æ—è¾¹å¤§å¦ˆä¸€ç›´é—®'è¿™æ˜¯åœ¨å“ªæ‹çš„'æœ‰ç‚¹å½±å“è§‚å½±ä½“éªŒã€‚",
            metadata={"source": "tweet"},
        )
        document_6 = Document(
            page_content="æ–°å‘å¸ƒçš„å°ç±³14Ultraå€¼ä¸å€¼å¾—ä¹°ï¼Ÿçœ‹å®Œè¿™ç¯‡æµ‹è¯„ä½ å°±çŸ¥é“ä¸ºä»€ä¹ˆæè€æ¿ç¬‘å¾—åˆä¸æ‹¢å˜´äº†ã€‚",
            metadata={"source": "website"},
        )
        document_7 = Document(
            page_content="2025å¹´ä¸­è¶…è”èµ›åå¤§æœ€ä½³çƒå‘˜æ¦œå•æ–°é²œå‡ºç‚‰ï¼Œç¬¬ä¸€åå±…ç„¶æ˜¯ä»–ï¼Ÿï¼",
            metadata={"source": "website"},
        )
        document_8 = Document(
            page_content="ç”¨LangChainå¼€å‘çš„AIåŠ©æ‰‹å¤ªç¥å¥‡äº†ï¼é—®å®ƒ'äººç”Ÿçš„æ„ä¹‰'ï¼Œå®ƒç»™æˆ‘æ¨èäº†ä¸€ä»½å¤–å–ä¼˜æƒ åˆ¸...",
            metadata={"source": "tweet"},
        )
        document_9 = Document(
            page_content="Aè‚¡ä»Šæ—¥æš´è·Œï¼Œåˆ†æå¸ˆç§°åŸå› æ˜¯'å¤§å®¶éƒ½åœ¨æŠ¢ç€å–'ï¼ŒæŠ•èµ„è€…è¡¨ç¤ºå¾ˆæœ‰é“ç†ã€‚",
            metadata={"source": "news"},
        )
        document_10 = Document(
            page_content="æ„Ÿè§‰æˆ‘é©¬ä¸Šè¦è¢«åˆ åº“è·‘è·¯äº†ï¼Œç¥æˆ‘å¥½è¿ /(ã„’oã„’)/~~",
            metadata={"source": "tweet"},
        )
        documents = [
            document_1,
            document_2,
            document_3,
            document_4,
            document_5,
            document_6,
            document_7,
            document_8,
            document_9,
            document_10,
        ]
        uuids = [str(uuid4()) for _ in range(len(documents))]
        vector_store.add_documents(documents=documents, ids=uuids)
        results = vector_store.similarity_search(
            "çœ‹ç”µå½±", #æœç´¢è¯
            k=1, # è¿”å›ç»“æœæ•°
            filter={"source": "tweet"}, # ç­›é€‰
        )
        for res in results:
            print(f"* {res.page_content} [{res.metadata}]")
        print("-"*50)

        vector_store.delete(ids=[uuids[-1]]) #åˆ é™¤

        results = vector_store.similarity_search_with_score(
            "æ˜å¤©çƒ­å—?", k=1, filter={"source": "news"}
        )
        for res, score in results:
            print(f"* [SIM={score:3f}] {res.page_content} [{res.metadata}]")
        print("-"*50)

        results = vector_store.max_marginal_relevance_search(
            query="æ–°æ‰‹æœº",
            k=1,
            lambda_val=0.8,
            filter={"source": "website"},
        )
        for res in results:
            print(f"*{res.page_content} [{res.metadata}]")
        print("-"*50)
    # vector2()


if __name__ == "__main__":
    main()
