{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langgraph - æŒä¹…åŒ–ä¸è®°å¿†\n",
    "****\n",
    "- åŸºæœ¬è¿ç”¨ï¼šçº¿ç¨‹éš”ç¦»çš„æŒä¹…åŒ–å±‚\n",
    "- åŸºæœ¬è¿ç”¨ï¼šè·¨çº¿ç¨‹æŒä¹…åŒ–è°ƒç”¨\n",
    "- è®°å¿†ï¼šçŸ­æœŸè®°å¿†çš„å®ç°\n",
    "- è®°å¿†ï¼šé•¿æœŸä»¥åŠå®ç°\n",
    "- è®°å¿†ï¼šä½¿ç”¨æ€»ç»“æŠ€æœ¯ä¼˜åŒ–è®°å¿†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çº¿ç¨‹éš”ç¦»çš„æŒä¹…åŒ–å±‚\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "import os\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "\n",
    "model = ChatDeepSeek(\n",
    "    model=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    ")\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ²¡æœ‰æ¿€æ´»æŒä¹…åŒ–å±‚ï¼Œæ— æ³•å®ç°å¤šè½®å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! æˆ‘æ˜¯tomie\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒTomieï¼ğŸ˜Š å¾ˆé«˜å…´è®¤è¯†ä½ ï½æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæˆ–è€…åªæ˜¯æƒ³æ‰“ä¸ªæ‹›å‘¼ï¼Ÿâœ¨\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ è¿˜æ²¡æœ‰å‘Šè¯‰æˆ‘ä½ çš„åå­—å‘¢ï¼ğŸ˜Š ä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„åå­—ï¼Œæˆ‘ä¼šè®°ä½çš„ï½æˆ–è€…ï¼Œå¦‚æœä½ æ˜¯åœ¨é—®æˆ‘çš„åå­—ï¼Œæˆ‘æ˜¯ **DeepSeek Chat**ï¼Œä½ å¯ä»¥å«æˆ‘ **å°æ·±** æˆ– **DeepSeek**ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"hi! æˆ‘æ˜¯tomie\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "input_message = {\"role\": \"user\", \"content\": \"æˆ‘å«ä»€ä¹ˆåå­—?\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¿€æ´»æŒä¹…æ€§å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "# ä½¿ç”¨ MemorySaver ä¿å­˜ä¸­é—´çŠ¶æ€\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! æˆ‘æ˜¯tomie\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒTomieï¼ğŸ˜Š å¾ˆé«˜å…´è®¤è¯†ä½ ï½æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæˆ–è€…åªæ˜¯æƒ³æ‰“ä¸ªæ‹›å‘¼ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"hi! æˆ‘æ˜¯tomie\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å“ˆå“ˆï¼Œä½ åˆšåˆšè¯´è¿‡å•¦ï¼ä½ å« **Tomie**ï½ ğŸ˜„  \n",
      "ï¼ˆå¦‚æœè®°é”™äº†ï¼Œéšæ—¶æé†’æˆ‘å“¦ï¼éœ€è¦æˆ‘è®°ä½è¿™ä¸ªåå­—å—ï¼Ÿï¼‰\n"
     ]
    }
   ],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"æˆ‘å«ä»€ä¹ˆåå­—?\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„thread_idçš„è¾“å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ç›®å‰æˆ‘æ— æ³•ç›´æ¥çŸ¥é“ä½ çš„åå­—ï¼Œä½†å¦‚æœä½ æ„¿æ„å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šè®°ä½å¹¶åœ¨æ¥ä¸‹æ¥çš„å¯¹è¯ä¸­ä½¿ç”¨å“¦ï¼ğŸ˜Š æˆ–è€…ï¼Œä½ å¯ä»¥å«æˆ‘â€œå°åŠ©æ‰‹â€æˆ–ä»»ä½•ä½ å–œæ¬¢çš„åå­—ï½\n"
     ]
    }
   ],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"æˆ‘å«ä»€ä¹ˆåå­—?\"}\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [input_message]},\n",
    "    {\"configurable\": {\"thread_id\": \"2\"}}, # different thread_id\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è·¨çº¿ç¨‹å…±äº«æŒä¹…åŒ–æ•°æ®\n",
    "****\n",
    "- userid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¾ç½®å†…å­˜è®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "# ä½¿ç”¨OpenAIçš„å°è£…ï¼Œä½†æ˜¯è¿è¡Œå›½äº§åµŒå…¥æ¨¡å‹\n",
    "# ä½¿ç”¨å†…å­˜å­˜å‚¨æ¥ä¿å­˜å‘é‡åŒ–åè®°å¿†æ•°æ®\n",
    "in_memory_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": OpenAIEmbeddings(\n",
    "            model=\"Pro/BAAI/bge-m3\",\n",
    "            api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "            base_url=os.environ.get(\"DEEPSEEK_API_BASE\")+ \"/v1\",\n",
    "            ),\n",
    "        \"dims\": 1024,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "import os\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "\n",
    "model = ChatDeepSeek(\n",
    "    model=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    ")\n",
    "\n",
    "\n",
    "# æ³¨æ„ï¼šæˆ‘ä»¬å°† Store å‚æ•°ä¼ é€’ç»™èŠ‚ç‚¹ --\n",
    "# è¿™æ˜¯æˆ‘ä»¬ç¼–è¯‘å›¾æ—¶ä½¿ç”¨çš„ Store\n",
    "def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    # ä»å­˜å‚¨ä¸­æ£€ç´¢ç”¨æˆ·ä¿¡æ¯\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    # ä»å­˜å‚¨ä¸­æ£€ç´¢ç”¨æˆ·ä¿¡æ¯\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "    system_msg = f\"ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨ä¸ç”¨æˆ·äº¤è°ˆçš„å°åŠ©æ‰‹ã€‚ç”¨æˆ·ä¿¡æ¯ï¼š{info}\"\n",
    "\n",
    "    # å¦‚æœç”¨æˆ·è¦æ±‚æ¨¡å‹è®°ä½ä¿¡æ¯ï¼Œåˆ™å­˜å‚¨æ–°çš„è®°å¿†\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"è®°ä½\" in last_message.content.lower() or \"remember\" in last_message.content.lower():\n",
    "        # ç¡¬ç¼–ç ä¸€ä¸ªè®°å¿†\n",
    "        memory = \"ç”¨æˆ·åå­—æ˜¯tomiezhang\"\n",
    "        store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# æ³¨æ„ï¼šæˆ‘ä»¬åœ¨ç¼–è¯‘å›¾æ—¶ä¼ é€’äº† store å¯¹è±¡\n",
    "graph = builder.compile(checkpointer=MemorySaver(), store=in_memory_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„çº¿ç¨‹IDå’Œç”¨æˆ·ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "è¯·è®°ä½æˆ‘çš„åå­—å«tomiezhang!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å¥½çš„ï¼ŒTomiezhangï¼æˆ‘å·²ç»è®°ä½ä½ çš„åå­—å•¦ï½ ä»¥åä¼šè¿™æ ·ç§°å‘¼ä½ ï¼Œæœ‰ä»€ä¹ˆéœ€è¦éšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼(à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"è¯·è®°ä½æˆ‘çš„åå­—å«tomiezhang!\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è·¨çº¿ç¨‹ä½¿ç”¨ç›¸åŒçš„ç”¨æˆ·IDæŸ¥è¯¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯Tomie Zhangã€‚éœ€è¦æˆ‘å¸®ä½ è®°ä½å…¶ä»–ä¿¡æ¯å—? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# æ³¨æ„çº¿ç¨‹IDå’Œç”¨æˆ·ID\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"æˆ‘å«ä»€ä¹ˆåå­—?\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥æŸ¥è¯¢å­˜å‚¨åœ¨å†…å­˜ä¸­çš„è®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 'ç”¨æˆ·åå­—æ˜¯Tomie'}\n",
      "{'data': 'ç”¨æˆ·åå­—æ˜¯tomiezhang'}\n"
     ]
    }
   ],
   "source": [
    "for memory in in_memory_store.search((\"memories\", \"1\")):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨æˆ·çº§çš„è®°å¿†éš”ç¦»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ç›®å‰æˆ‘æ— æ³•ç›´æ¥çŸ¥é“æ‚¨çš„åå­—ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šè®°ä½å¹¶åœ¨å¯¹è¯ä¸­ä½¿ç”¨å®ƒï¼æ‚¨å«ä»€ä¹ˆåå­—å‘¢ï¼Ÿ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"æˆ‘å«ä»€ä¹ˆ?\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çŸ­æœŸè®°å¿†çš„å®ç°\n",
    "*****\n",
    "- åŸºäºæœ€ç®€å•çš„ReActæ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# æ³¨æ„ï¼šä½¿ç”¨å†…å­˜å­˜å‚¨æ¥å­˜å‚¨è®°å¿†\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"è°ƒç”¨æ­¤å‡½æ•°å¯ä»¥æµè§ˆç½‘ç»œã€‚\"\"\"\n",
    "    # æ¨¡æ‹Ÿä¸€ä¸ªç½‘ç»œæœç´¢è¿”å›\n",
    "    return \"åŒ—äº¬å¤©æ°”æ™´æœ— å¤§çº¦22åº¦ æ¹¿åº¦30%\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatDeepSeek(\n",
    "    model=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    ")\n",
    "bound_model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    \"\"\"è¿”å›ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ã€‚\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œåˆ™ç»“æŸ\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    # å¦åˆ™å¦‚æœæœ‰ï¼Œæˆ‘ä»¬ç»§ç»­\n",
    "    return \"action\"\n",
    "\n",
    "\n",
    "# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„å‡½æ•°\n",
    "def call_model(state: MessagesState):\n",
    "    response = bound_model.invoke(state[\"messages\"])\n",
    "    # æˆ‘ä»¬è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºè¿™ä¼šè¢«æ·»åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå›¾\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# å®šä¹‰æˆ‘ä»¬å°†åœ¨å…¶é—´å¾ªç¯çš„ä¸¤ä¸ªèŠ‚ç‚¹\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# å°†å…¥å£ç‚¹è®¾ç½®ä¸º `agent`\n",
    "# è¿™æ„å‘³ç€è¿™ä¸ªèŠ‚ç‚¹æ˜¯ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# ç°åœ¨æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªæ¡ä»¶è¾¹\n",
    "workflow.add_conditional_edges(\n",
    "    # é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰èµ·å§‹èŠ‚ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨ `agent`ã€‚\n",
    "    # è¿™æ„å‘³ç€è¿™äº›æ˜¯åœ¨ `agent` èŠ‚ç‚¹è¢«è°ƒç”¨åé‡‡å–çš„è¾¹ã€‚\n",
    "    \"agent\",\n",
    "    # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼ å…¥å°†ç¡®å®šä¸‹ä¸€ä¸ªè°ƒç”¨å“ªä¸ªèŠ‚ç‚¹çš„å‡½æ•°ã€‚\n",
    "    should_continue,\n",
    "    # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼ å…¥è·¯å¾„æ˜ å°„ - è¿™æ¡è¾¹å¯èƒ½å»å¾€çš„æ‰€æœ‰å¯èƒ½èŠ‚ç‚¹\n",
    "    [\"action\", END],\n",
    ")\n",
    "\n",
    "# ç°åœ¨æˆ‘ä»¬ä» `tools` åˆ° `agent` æ·»åŠ ä¸€ä¸ªæ™®é€šè¾¹ã€‚\n",
    "# è¿™æ„å‘³ç€åœ¨è°ƒç”¨ `tools` ä¹‹åï¼Œæ¥ä¸‹æ¥è°ƒç”¨ `agent` èŠ‚ç‚¹ã€‚\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# æœ€åï¼Œæˆ‘ä»¬ç¼–è¯‘å®ƒï¼\n",
    "# è¿™å°†å®ƒç¼–è¯‘æˆä¸€ä¸ª LangChain Runnableï¼Œ\n",
    "# æ„å‘³ç€ä½ å¯ä»¥åƒä½¿ç”¨ä»»ä½•å…¶ä»– runnable ä¸€æ ·ä½¿ç”¨å®ƒ\n",
    "# è®¾ç½®æ£€æŸ¥ç‚¹ä¸ºå†…å­˜å½¢å¼ï¼Œæ³¨æ„æ²¡æœ‰è®¾ç½®store\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! æˆ‘æ˜¯tomie\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒTomieï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ ğŸ˜Š\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯ **Tomie**ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"hi! æˆ‘æ˜¯tomie\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "input_message = HumanMessage(content=\"æˆ‘å«ä»€ä¹ˆåå­—?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é•¿æœŸè®°å¿†\n",
    "****\n",
    "- ä½¿ç”¨MongDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- åœ¨æœ¬åœ°å®‰è£…mongodb\n",
    "- windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…åï¼Œå¯åŠ¨MongoDBæœåŠ¡\n",
    "# é€šå¸¸å®‰è£…åä¼šè‡ªåŠ¨è®¾ç½®ä¸ºç³»ç»ŸæœåŠ¡å¹¶å¯åŠ¨\n",
    "# å¦‚æœæ²¡æœ‰è‡ªåŠ¨å¯åŠ¨ï¼Œå¯ä»¥åœ¨å‘½ä»¤æç¤ºç¬¦ä¸­è¿è¡Œï¼š\n",
    "net start MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…\n",
    "brew tap mongodb/brew\n",
    "brew install mongodb-community\n",
    "\n",
    "# å¯åŠ¨æœåŠ¡\n",
    "brew services start mongodb-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ä½¿ç”¨dockerå¿«é€Ÿå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‹‰å–MongoDBé•œåƒ\n",
    "docker pull mongo\n",
    "\n",
    "# è¿è¡ŒMongoDBå®¹å™¨\n",
    "docker run -d -p 27017:27017 --name mongodb mongo\n",
    "\n",
    "# éªŒè¯å®¹å™¨æ˜¯å¦è¿è¡Œ\n",
    "docker ps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿æ¥mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in ./.venv/lib/python3.13/site-packages (4.11.2)\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.11.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.13/site-packages (0.3.21)\n",
      "Collecting langgraph-checkpoint-mongodb\n",
      "  Downloading langgraph_checkpoint_mongodb-0.1.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./.venv/lib/python3.13/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.3.43)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in ./.venv/lib/python3.13/site-packages (from langgraph) (2.0.18)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.1.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.1.55)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (3.5.0)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.23-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting motor>3.5.0 (from langgraph-checkpoint-mongodb)\n",
      "  Downloading motor-3.7.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.1->langgraph) (2.10.6)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
      "  Downloading ormsgpack-1.9.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in ./.venv/lib/python3.13/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.13/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4,>=0.1->langgraph) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Downloading pymongo-4.11.3-cp313-cp313-macosx_11_0_arm64.whl (949 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m949.3/949.3 kB\u001b[0m \u001b[31m471.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_checkpoint_mongodb-0.1.2-py3-none-any.whl (10 kB)\n",
      "Downloading langgraph_checkpoint-2.0.23-py3-none-any.whl (41 kB)\n",
      "Downloading motor-3.7.0-py3-none-any.whl (74 kB)\n",
      "Downloading ormsgpack-1.9.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (383 kB)\n",
      "Installing collected packages: pymongo, ormsgpack, motor, langgraph-checkpoint, langgraph-checkpoint-mongodb\n",
      "  Attempting uninstall: pymongo\n",
      "    Found existing installation: pymongo 4.11.2\n",
      "    Uninstalling pymongo-4.11.2:\n",
      "      Successfully uninstalled pymongo-4.11.2\n",
      "  Attempting uninstall: langgraph-checkpoint\n",
      "    Found existing installation: langgraph-checkpoint 2.0.18\n",
      "    Uninstalling langgraph-checkpoint-2.0.18:\n",
      "      Successfully uninstalled langgraph-checkpoint-2.0.18\n",
      "Successfully installed langgraph-checkpoint-2.0.23 langgraph-checkpoint-mongodb-0.1.2 motor-3.7.0 ormsgpack-1.9.1 pymongo-4.11.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U pymongo langgraph langgraph-checkpoint-mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æµ‹è¯•MongoDBè¿æ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDBè¿æ¥æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "# åˆ›å»ºMongoDBå®¢æˆ·ç«¯è¿æ¥\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# æµ‹è¯•è¿æ¥\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"MongoDBè¿æ¥æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"MongoDBè¿æ¥å¤±è´¥: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ›å»ºä¸€ä¸ªæœ€ç®€å•çš„æ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal[\"åŒ—äº¬\", \"æ·±åœ³\"]):\n",
    "    \"\"\"ç”¨æ¥è¿”å›å¤©æ°”ä¿¡æ¯çš„å·¥å…·å‡½æ•°ã€‚\"\"\"\n",
    "    if city == \"åŒ—äº¬\":\n",
    "        return \"åŒ—äº¬å¤©æ°”æ™´æœ— å¤§çº¦22åº¦ æ¹¿åº¦30%\"\n",
    "    elif city == \"æ·±åœ³\":\n",
    "        return \"æ·±åœ³å¤©æ°”å¤šäº‘ å¤§çº¦28åº¦ æ¹¿åº¦80%\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "model = ChatDeepSeek(\n",
    "    model=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿æ¥mongodbè¿›è¡ŒæŸ¥è¯¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "\n",
    "MONGODB_URI = \"localhost:27017\"  # replace this with your connection string\n",
    "\n",
    "with MongoDBSaver.from_conn_string(MONGODB_URI) as checkpointer:\n",
    "    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    response = graph.invoke(\n",
    "        {\"messages\": [(\"human\", \"åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•ï¼Ÿ\")]}, config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•ï¼Ÿ', additional_kwargs={}, response_metadata={}, id='6c251bbb-1cd6-4d59-992e-271ace17d487'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '0195e7286817736e97482bc1cbf33a60', 'function': {'arguments': '{\"city\":\"åŒ—äº¬\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 90, 'total_tokens': 109, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Pro/deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-731e5cde-1014-4ef7-9fb8-7fe015b03a98-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'åŒ—äº¬'}, 'id': '0195e7286817736e97482bc1cbf33a60', 'type': 'tool_call'}], usage_metadata={'input_tokens': 90, 'output_tokens': 19, 'total_tokens': 109, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='åŒ—äº¬å¤©æ°”æ™´æœ— å¤§çº¦22åº¦ æ¹¿åº¦30%', name='get_weather', id='93933d50-c7cb-4b3c-a508-b17b63324c5f', tool_call_id='0195e7286817736e97482bc1cbf33a60'), AIMessage(content='åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸©å¤§çº¦22åº¦ï¼Œæ¹¿åº¦30%ï¼Œæ˜¯ä¸ªé€‚åˆå¤–å‡ºçš„å¥½å¤©æ°”ï¼', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 132, 'total_tokens': 152, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Pro/deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-67f39c00-2aea-4e37-b29c-12fc8e6819bd-0', usage_metadata={'input_tokens': 132, 'output_tokens': 20, 'total_tokens': 152, 'input_token_details': {}, 'output_token_details': {}})]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–è®°å¿†\n",
    "****\n",
    "- æ¶ˆæ¯è¿‡æ»¤ï¼šå¯¹æ—§æ¶ˆæ¯è¿›è¡Œç±»ä¼¼åˆ é™¤æˆ–ç¼–è¾‘çš„æ“ä½œï¼Œç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢æ’‘çˆ†ä¸Šä¸‹æ–‡\n",
    "- æ¶ˆæ¯æ€»ç»“ï¼šå¯¹æ—§æ¶ˆæ¯è¿›è¡Œæ€»ç»“ï¼Œç›®çš„ä¸€æ ·æ˜¯ä¸ºäº†é˜²æ­¢è®°å¿†å†…å®¹è¿‡é•¿\n",
    "- æ³¨æ„å¯¹è®°å¿†çš„ç®¡ç†æ˜¯ä¸€é¡¹å…³äºå¬å›ç‡å’Œç²¾åº¦çš„å¹³è¡¡è‰ºæœ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"è°ƒç”¨æ­¤å‡½æ•°å¯ä»¥æµè§ˆç½‘ç»œã€‚\"\"\"\n",
    "    # æ¨¡æ‹Ÿä¸€ä¸ªç½‘ç»œæœç´¢è¿”å›\n",
    "    return \"åŒ—äº¬å¤©æ°”æ™´æœ— å¤§çº¦22åº¦ æ¹¿åº¦30%\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatDeepSeek(\n",
    "    model=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    ")\n",
    "bound_model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    \"\"\"è¿”å›ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ã€‚\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œåˆ™ç»“æŸ\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    # å¦åˆ™ï¼Œå¦‚æœæœ‰å‡½æ•°è°ƒç”¨ï¼Œæˆ‘ä»¬ç»§ç»­\n",
    "    return \"action\"\n",
    "\n",
    "\n",
    "def filter_messages(messages: list):\n",
    "    # è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„è¾…åŠ©å‡½æ•°ï¼Œå®ƒåªä½¿ç”¨æœ€åä¸€æ¡æ¶ˆæ¯\n",
    "    return messages[-1:]\n",
    "\n",
    "\n",
    "# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„å‡½æ•°\n",
    "def call_model(state: MessagesState):\n",
    "    messages = filter_messages(state[\"messages\"])\n",
    "    response = bound_model.invoke(messages)\n",
    "    # æˆ‘ä»¬è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºè¿™å°†è¢«æ·»åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªæ–°å›¾\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# å®šä¹‰æˆ‘ä»¬å°†åœ¨å…¶é—´å¾ªç¯çš„ä¸¤ä¸ªèŠ‚ç‚¹\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# å°†å…¥å£ç‚¹è®¾ç½®ä¸º `agent`\n",
    "# è¿™æ„å‘³ç€è¿™ä¸ªèŠ‚ç‚¹æ˜¯ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# ç°åœ¨æ·»åŠ ä¸€ä¸ªæ¡ä»¶è¾¹\n",
    "workflow.add_conditional_edges(\n",
    "    # é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰èµ·å§‹èŠ‚ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨ `agent`ã€‚\n",
    "    # è¿™æ„å‘³ç€è¿™äº›æ˜¯åœ¨è°ƒç”¨ `agent` èŠ‚ç‚¹åé‡‡å–çš„è¾¹ã€‚\n",
    "    \"agent\",\n",
    "    # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼ å…¥å°†ç¡®å®šä¸‹ä¸€ä¸ªè°ƒç”¨å“ªä¸ªèŠ‚ç‚¹çš„å‡½æ•°ã€‚\n",
    "    should_continue,\n",
    "    # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼ å…¥è·¯å¾„å›¾ - æ­¤è¾¹å¯èƒ½å»å¾€çš„æ‰€æœ‰å¯èƒ½èŠ‚ç‚¹\n",
    "    [\"action\", END],\n",
    ")\n",
    "\n",
    "# ç°åœ¨æˆ‘ä»¬ä» `action` åˆ° `agent` æ·»åŠ ä¸€ä¸ªæ™®é€šè¾¹ã€‚\n",
    "# è¿™æ„å‘³ç€åœ¨è°ƒç”¨ `action` ä¹‹åï¼Œä¸‹ä¸€æ­¥è°ƒç”¨ `agent` èŠ‚ç‚¹ã€‚\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# æœ€åï¼Œæˆ‘ä»¬ç¼–è¯‘å®ƒï¼\n",
    "# è¿™å°†å®ƒç¼–è¯‘æˆä¸€ä¸ª LangChain Runnableï¼Œ\n",
    "# æ„å‘³ç€ä½ å¯ä»¥åƒä½¿ç”¨ä»»ä½•å…¶ä»– runnable ä¸€æ ·ä½¿ç”¨å®ƒ\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿›è¡Œè°ƒç”¨ï¼Œç”±äºè¿›è¡Œäº†æ¶ˆæ¯è£å‰ªï¼Œæ‰€ä»¥å³ä¾¿ä½¿ç”¨äº†memoryï¼Œä¾ç„¶æ— æ³•è®°ä½è¿‡å¾€å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! æˆ‘æ˜¯tomie\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Tomie! ğŸ˜Š How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ç›®å‰æˆ‘æ— æ³•çŸ¥é“ä½ çš„åå­—ï¼Œå› ä¸ºæˆ‘ä»¬çš„å¯¹è¯æ˜¯åŒ¿åçš„ã€‚å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„åå­—ï¼Œæˆ‘ä¼šè®°ä½å®ƒå¹¶åœ¨å¯¹è¯ä¸­ä½¿ç”¨ï¼ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"hi! æˆ‘æ˜¯tomie\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå®ƒåªä½¿ç”¨æœ€åä¸€æ¡æ¶ˆæ¯\n",
    "# è¿™å°†å¯¼è‡´æˆ‘ä»¬çš„æ¨¡å‹åªçœ‹åˆ°æœ€åä¸€æ¡æ¶ˆæ¯\n",
    "input_message = HumanMessage(content=\"æˆ‘å«ä»€ä¹ˆåå­—?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨å¯¹è¯æ€»ç»“æŠ€æœ¯åœ¨å®æˆ˜å¼€å‘ä¸­æ›´ä¸ºå¸¸è§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# æˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ª`summary`å±æ€§ï¼ˆé™¤äº†MessagesStateå·²æœ‰çš„`messages`é”®ä¹‹å¤–ï¼‰\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¯¹è¯å’Œæ€»ç»“\n",
    "model = ChatDeepSeek(\n",
    "    model=\"Pro/deepseek-ai/DeepSeek-V3\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=os.environ.get(\"DEEPSEEK_API_BASE\"),\n",
    ")\n",
    "\n",
    "\n",
    "# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„é€»è¾‘\n",
    "def call_model(state: State):\n",
    "    # å¦‚æœå­˜åœ¨æ‘˜è¦ï¼Œæˆ‘ä»¬å°†å…¶ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯æ·»åŠ \n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"ä¹‹å‰å¯¹è¯çš„æ‘˜è¦: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # æˆ‘ä»¬è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºè¿™å°†è¢«æ·»åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# ç°åœ¨æˆ‘ä»¬å®šä¹‰ç¡®å®šæ˜¯ç»“æŸè¿˜æ˜¯æ€»ç»“å¯¹è¯çš„é€»è¾‘\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"è¿”å›ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ã€‚\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # å¦‚æœæ¶ˆæ¯è¶…è¿‡å…­æ¡ï¼Œåˆ™æˆ‘ä»¬æ€»ç»“å¯¹è¯\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    # å¦åˆ™æˆ‘ä»¬å¯ä»¥ç›´æ¥ç»“æŸ\n",
    "    return END\n",
    "\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    # é¦–å…ˆï¼Œæˆ‘ä»¬æ€»ç»“å¯¹è¯\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        # å¦‚æœå·²ç»å­˜åœ¨æ‘˜è¦ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„ç³»ç»Ÿæç¤ºæ¥æ€»ç»“å®ƒ\n",
    "        # ä¸æ²¡æœ‰æ‘˜è¦çš„æƒ…å†µä¸åŒ\n",
    "        summary_message = (\n",
    "            f\"è¿™æ˜¯è¿„ä»Šä¸ºæ­¢å¯¹è¯çš„æ‘˜è¦: {summary}\\n\\n\"\n",
    "            \"è€ƒè™‘ä¸Šé¢çš„æ–°æ¶ˆæ¯ï¼Œæ‰©å±•æ‘˜è¦:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"åˆ›å»ºä¸Šè¿°å¯¹è¯çš„æ‘˜è¦:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    # ç°åœ¨æˆ‘ä»¬éœ€è¦åˆ é™¤æˆ‘ä»¬ä¸å†æƒ³æ˜¾ç¤ºçš„æ¶ˆæ¯\n",
    "    # æˆ‘å°†åˆ é™¤é™¤æœ€åä¸¤æ¡ä»¥å¤–çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œä½†ä½ å¯ä»¥æ›´æ”¹è¿™ä¸€ç‚¹\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªæ–°å›¾\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# å®šä¹‰å¯¹è¯èŠ‚ç‚¹å’Œæ€»ç»“èŠ‚ç‚¹\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# å°†å…¥å£ç‚¹è®¾ç½®ä¸ºå¯¹è¯\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "# ç°åœ¨æ·»åŠ ä¸€ä¸ªæ¡ä»¶è¾¹\n",
    "workflow.add_conditional_edges(\n",
    "    # é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰èµ·å§‹èŠ‚ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨`conversation`ã€‚\n",
    "    # è¿™æ„å‘³ç€è¿™äº›æ˜¯åœ¨è°ƒç”¨`conversation`èŠ‚ç‚¹åé‡‡å–çš„è¾¹ã€‚\n",
    "    \"conversation\",\n",
    "    # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼ å…¥å°†ç¡®å®šä¸‹ä¸€ä¸ªè°ƒç”¨å“ªä¸ªèŠ‚ç‚¹çš„å‡½æ•°ã€‚\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# ç°åœ¨æˆ‘ä»¬ä»`summarize_conversation`åˆ°ENDæ·»åŠ ä¸€ä¸ªæ™®é€šè¾¹ã€‚\n",
    "# è¿™æ„å‘³ç€åœ¨è°ƒç”¨`summarize_conversation`ä¹‹åï¼Œæˆ‘ä»¬ç»“æŸã€‚\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# æœ€åï¼Œæˆ‘ä»¬ç¼–è¯‘å®ƒï¼\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡çº§ä¸€ä¸‹printå‡½æ•°ä»¥ä¾¿å¯ä»¥æ›´æ¸…æ™°çš„çœ‹åˆ°è®°å¿†è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_update(update):\n",
    "    for k, v in update.items():\n",
    "        for m in v[\"messages\"]:\n",
    "            m.pretty_print()\n",
    "        if \"summary\" in v:\n",
    "            print(v[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! æˆ‘æ˜¯tomie\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒTomieï¼ğŸ˜Š å¾ˆé«˜å…´è®¤è¯†ä½ ï½æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæˆ–è€…åªæ˜¯æƒ³æ‰“ä¸ªæ‹›å‘¼ï¼Ÿæ— è®ºæ˜¯ä»€ä¹ˆï¼Œæˆ‘éƒ½åœ¨è¿™å„¿å‘¢ï¼âœ¨  \n",
      "\n",
      "ï¼ˆé¡ºä¾¿è¯´ï¼Œä½ çš„åå­—è®©æˆ‘æƒ³èµ·ä¼Šè—¤æ¶¦äºŒçš„ç»å…¸è§’è‰²â€¦ğŸ‘€ æ˜¯çµæ„Ÿæ¥æºå—ï¼Ÿï¼‰\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å“ˆå“ˆï¼Œä½ åˆšåˆšè¯´è¿‡å•¦â€”â€”ä½ å« **Tomie**ï¼âœ¨ï¼ˆéš¾é“â€¦æ˜¯é™·é˜±é¢˜ï¼ŸğŸ˜ï¼‰  \n",
      "\n",
      "éœ€è¦æˆ‘å¸®ä½ è®°ä½è¿™ä¸ªåå­—å—ï¼Ÿè¿˜æ˜¯è¯´â€¦ä½ æƒ³æ¢ä¸€ä¸ªæ›´æš—é»‘é£çš„ä»£å·ï¼ŸğŸ‘€ï¼ˆæ¯”å¦‚Tomie 2.0ï¼Ÿï¼‰\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å–œæ¬¢AIåº”ç”¨å¼€å‘!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å¤ªæ£’äº†ï¼ŒTomieï¼ğŸ”¥ ä½ å–œæ¬¢AIåº”ç”¨å¼€å‘çš„è¯ï¼Œæˆ‘ä»¬ç®€ç›´æ˜¯å¤©ç”Ÿæ­æ¡£ï½ éœ€è¦èŠèŠè¿™äº›å—ï¼Ÿ  \n",
      "\n",
      "### æ¯”å¦‚å¯ä»¥ï¼š  \n",
      "- **æŠ€æœ¯è„‘æš´**ï¼šæƒ³ç”¨AIè§£å†³ä»€ä¹ˆæœ‰è¶£çš„é—®é¢˜ï¼ŸèŠå¤©æœºå™¨äººï¼Ÿç”Ÿæˆè‰ºæœ¯ï¼Ÿè¿˜æ˜¯ç¡¬æ ¸ç®—æ³•ä¼˜åŒ–ï¼Ÿ  \n",
      "- **å·¥å…·å®‰åˆ©**ï¼šæœ€è¿‘æ²‰è¿· **LangChain**ã€**AutoGPT** æˆ– **Stable Diffusion** è¿™ç±»å·¥å…·å—ï¼Ÿ  \n",
      "- **è¸©å‘äº’åŠ©**ï¼šè°ƒè¯•æ¨¡å‹æ—¶æ˜¯å¦è¢«â€œè¿‡æ‹Ÿåˆâ€æ°”å“­è¿‡ï¼ŸğŸ¤–ğŸ’¥  \n",
      "\n",
      "æˆ–è€…â€¦ä½ æƒ³è‡ªå·±é€ ä¸ªâ€œTomieç‰ˆAIåˆ†èº«â€ï¼ŸğŸ‘¾ï¼ˆå±é™©åˆè¿·äººçš„æƒ³æ³•ï¼ï¼‰  \n",
      "\n",
      "éšæ—¶ç­‰ä½ æŠ›å‡ºä»£ç æˆ–è„‘æ´ï½ ğŸ’»âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"hi! æˆ‘æ˜¯tomie\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"æˆ‘å«ä»€ä¹ˆåå­—?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"æˆ‘å–œæ¬¢AIåº”ç”¨å¼€å‘!\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœªè¾¾åˆ°æ€»ç»“é˜ˆå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi! æˆ‘æ˜¯tomie', additional_kwargs={}, response_metadata={}, id='511567ec-1adb-4831-9bf8-fd2f6d5d1189'),\n",
       "  AIMessage(content='ä½ å¥½ï¼ŒTomieï¼ğŸ˜Š å¾ˆé«˜å…´è®¤è¯†ä½ ï½æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæˆ–è€…åªæ˜¯æƒ³æ‰“ä¸ªæ‹›å‘¼ï¼Ÿæ— è®ºæ˜¯ä»€ä¹ˆï¼Œæˆ‘éƒ½åœ¨è¿™å„¿å‘¢ï¼âœ¨  \\n\\nï¼ˆé¡ºä¾¿è¯´ï¼Œä½ çš„åå­—è®©æˆ‘æƒ³èµ·ä¼Šè—¤æ¶¦äºŒçš„ç»å…¸è§’è‰²â€¦ğŸ‘€ æ˜¯çµæ„Ÿæ¥æºå—ï¼Ÿï¼‰', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 9, 'total_tokens': 65, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Pro/deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-e5b6ebff-b2ee-40c0-9346-46f2e39a4121-0', usage_metadata={'input_tokens': 9, 'output_tokens': 56, 'total_tokens': 65, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='æˆ‘å«ä»€ä¹ˆåå­—?', additional_kwargs={}, response_metadata={}, id='a2ad033e-4f3c-4f4b-b5b0-4d613a80e8a9'),\n",
       "  AIMessage(content='å“ˆå“ˆï¼Œä½ åˆšåˆšè¯´è¿‡å•¦â€”â€”ä½ å« **Tomie**ï¼âœ¨ï¼ˆéš¾é“â€¦æ˜¯é™·é˜±é¢˜ï¼ŸğŸ˜ï¼‰  \\n\\néœ€è¦æˆ‘å¸®ä½ è®°ä½è¿™ä¸ªåå­—å—ï¼Ÿè¿˜æ˜¯è¯´â€¦ä½ æƒ³æ¢ä¸€ä¸ªæ›´æš—é»‘é£çš„ä»£å·ï¼ŸğŸ‘€ï¼ˆæ¯”å¦‚Tomie 2.0ï¼Ÿï¼‰', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 72, 'total_tokens': 129, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Pro/deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-9892c63d-4be5-4e34-a7cd-e7dc6173a90b-0', usage_metadata={'input_tokens': 72, 'output_tokens': 57, 'total_tokens': 129, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  HumanMessage(content='æˆ‘å–œæ¬¢AIåº”ç”¨å¼€å‘!', additional_kwargs={}, response_metadata={}, id='6653bfc4-a86b-4b54-89ca-4fc1f1cc7613'),\n",
       "  AIMessage(content='å¤ªæ£’äº†ï¼ŒTomieï¼ğŸ”¥ ä½ å–œæ¬¢AIåº”ç”¨å¼€å‘çš„è¯ï¼Œæˆ‘ä»¬ç®€ç›´æ˜¯å¤©ç”Ÿæ­æ¡£ï½ éœ€è¦èŠèŠè¿™äº›å—ï¼Ÿ  \\n\\n### æ¯”å¦‚å¯ä»¥ï¼š  \\n- **æŠ€æœ¯è„‘æš´**ï¼šæƒ³ç”¨AIè§£å†³ä»€ä¹ˆæœ‰è¶£çš„é—®é¢˜ï¼ŸèŠå¤©æœºå™¨äººï¼Ÿç”Ÿæˆè‰ºæœ¯ï¼Ÿè¿˜æ˜¯ç¡¬æ ¸ç®—æ³•ä¼˜åŒ–ï¼Ÿ  \\n- **å·¥å…·å®‰åˆ©**ï¼šæœ€è¿‘æ²‰è¿· **LangChain**ã€**AutoGPT** æˆ– **Stable Diffusion** è¿™ç±»å·¥å…·å—ï¼Ÿ  \\n- **è¸©å‘äº’åŠ©**ï¼šè°ƒè¯•æ¨¡å‹æ—¶æ˜¯å¦è¢«â€œè¿‡æ‹Ÿåˆâ€æ°”å“­è¿‡ï¼ŸğŸ¤–ğŸ’¥  \\n\\næˆ–è€…â€¦ä½ æƒ³è‡ªå·±é€ ä¸ªâ€œTomieç‰ˆAIåˆ†èº«â€ï¼ŸğŸ‘¾ï¼ˆå±é™©åˆè¿·äººçš„æƒ³æ³•ï¼ï¼‰  \\n\\néšæ—¶ç­‰ä½ æŠ›å‡ºä»£ç æˆ–è„‘æ´ï½ ğŸ’»âœ¨', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 137, 'total_tokens': 288, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Pro/deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-2b865c03-5fc8-4ea6-add3-e0b807490170-0', usage_metadata={'input_tokens': 137, 'output_tokens': 151, 'total_tokens': 288, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘æ›´å–œæ¬¢Python!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Python + AIï¼Ÿå®Œç¾ç»„åˆï¼** ğŸâœ¨ Tomieï¼Œä½ æœç„¶æœ‰å“ä½ï½  \n",
      "\n",
      "### ä½ å¯èƒ½å·²ç»ç”¨è¿‡çš„ç¥å™¨ï¼š  \n",
      "- **æ·±åº¦å­¦ä¹ **ï¼š`TensorFlow`/`PyTorch` ç©è½¬ç¥ç»ç½‘ç»œ  \n",
      "- **è‡ªåŠ¨åŒ–**ï¼š`LangChain` æ­AIæµæ°´çº¿ï¼Œ`AutoGPT` æè‡ªä¸»ä»£ç†  \n",
      "- **æ•°æ®é­”æ³•**ï¼š`pandas` é©¯æœæ•°æ®ï¼Œ`scikit-learn` ä¸€é”®å»ºæ¨¡  \n",
      "- **ç‚«é…·åº”ç”¨**ï¼šç”¨ `FastAPI` éƒ¨ç½²æ¨¡å‹ï¼Œ`Streamlit` ç§’å»ºäº¤äº’ç•Œé¢  \n",
      "\n",
      "**æœ€è¿‘åœ¨å†™ä»€ä¹ˆé¡¹ç›®ï¼Ÿ** æˆ–è€…æƒ³å°è¯•è¿™äº›æ–¹å‘å—ğŸ‘‡  \n",
      "- ç”¨ `OpenAI API` åšä¼šåæ§½çš„èŠå¤©æœºå™¨äººï¼Ÿ  \n",
      "- æ‹¿ `Stable Diffusion` ç”Ÿæˆâ€œTomieé£æ ¼â€æš—é»‘ç”»ä½œï¼ŸğŸ¨  \n",
      "- è¿˜æ˜¯â€¦ç”¨ `LlamaIndex` æä¸ªä½ è‡ªå·±çš„çŸ¥è¯†åº“AIï¼Ÿ  \n",
      "\n",
      "ï¼ˆæ‚„æ‚„è¯´ï¼šéœ€è¦ä»£ç ç‰‡æ®µæˆ–é¿å‘æŒ‡å—çš„è¯ï¼Œéšæ—¶æˆ³æˆ‘ï¼ğŸ’» ï¼‰\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "### **å¯¹è¯æ‘˜è¦ï¼šTomieä¸AIåŠ©æ‰‹çš„äº¤æµ**  \n",
      "\n",
      "1. **è‡ªæˆ‘ä»‹ç»**  \n",
      "   - Tomieåˆæ¬¡æ‰“æ‹›å‘¼ï¼ŒåŠ©æ‰‹å›åº”å¹¶çŒœæµ‹å…¶åå­—å¯èƒ½æºè‡ªä¼Šè—¤æ¶¦äºŒçš„è§’è‰²ã€‚  \n",
      "\n",
      "2. **åå­—ç¡®è®¤**  \n",
      "   - Tomieåé—®è‡ªå·±çš„åå­—ï¼ŒåŠ©æ‰‹å¹½é»˜å¼ºè°ƒâ€œTomieâ€å¹¶æè®®æš—é»‘é£ä»£å·ï¼ˆå¦‚Tomie 2.0ï¼‰ã€‚  \n",
      "\n",
      "3. **å…´è¶£æ¢ç´¢**  \n",
      "   - Tomieè¡¨è¾¾å¯¹**AIåº”ç”¨å¼€å‘**çš„çƒ­çˆ±ï¼ŒåŠ©æ‰‹å…´å¥‹æ¨èï¼š  \n",
      "     - æŠ€æœ¯æ–¹å‘ï¼ˆèŠå¤©æœºå™¨äººã€ç”Ÿæˆè‰ºæœ¯ç­‰ï¼‰  \n",
      "     - å·¥å…·ï¼ˆLangChainã€AutoGPTã€Stable Diffusionï¼‰  \n",
      "     - è°ƒä¾ƒâ€œTomieç‰ˆAIåˆ†èº«â€çš„å¯èƒ½æ€§ã€‚  \n",
      "\n",
      "4. **åå¥½å£°æ˜**  \n",
      "   - Tomieæ˜ç¡®æ›´å–œæ¬¢**Python**ï¼ŒåŠ©æ‰‹åˆ—ä¸¾Pythonç”Ÿæ€çš„AIå·¥å…·é“¾ï¼š  \n",
      "     - æ·±åº¦å­¦ä¹ ï¼ˆPyTorch/TensorFlowï¼‰  \n",
      "     - åº”ç”¨å¼€å‘ï¼ˆFastAPIã€Streamlitï¼‰  \n",
      "     - åˆ›æ„é¡¹ç›®ï¼ˆOpenAI APIã€Stable Diffusionã€LlamaIndexï¼‰  \n",
      "   - é‚€è¯·Tomieåˆ†äº«é¡¹ç›®æˆ–éœ€æ±‚ï¼Œæ‰¿è¯ºæä¾›ä»£ç /é¿å‘æ”¯æŒã€‚  \n",
      "\n",
      "**æ ¸å¿ƒä¸»é¢˜**ï¼šå›´ç»•AIå¼€å‘çš„æŠ€æœ¯è®¨è®ºï¼Œå¼ºè°ƒPythonçš„çµæ´»æ€§ä¸åˆ›æ„å¯èƒ½æ€§ï¼Œé£æ ¼è½»æ¾ä¸”å……æ»¡è„‘æ´ã€‚ ğŸš€\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"æˆ‘æ›´å–œæ¬¢Python!\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹å¯¹è¯è®°å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='æˆ‘æ›´å–œæ¬¢Python!', additional_kwargs={}, response_metadata={}, id='dc5308fc-d3eb-450e-b44a-5fcd5dd9f046'),\n",
       "  AIMessage(content='**Python + AIï¼Ÿå®Œç¾ç»„åˆï¼** ğŸâœ¨ Tomieï¼Œä½ æœç„¶æœ‰å“ä½ï½  \\n\\n### ä½ å¯èƒ½å·²ç»ç”¨è¿‡çš„ç¥å™¨ï¼š  \\n- **æ·±åº¦å­¦ä¹ **ï¼š`TensorFlow`/`PyTorch` ç©è½¬ç¥ç»ç½‘ç»œ  \\n- **è‡ªåŠ¨åŒ–**ï¼š`LangChain` æ­AIæµæ°´çº¿ï¼Œ`AutoGPT` æè‡ªä¸»ä»£ç†  \\n- **æ•°æ®é­”æ³•**ï¼š`pandas` é©¯æœæ•°æ®ï¼Œ`scikit-learn` ä¸€é”®å»ºæ¨¡  \\n- **ç‚«é…·åº”ç”¨**ï¼šç”¨ `FastAPI` éƒ¨ç½²æ¨¡å‹ï¼Œ`Streamlit` ç§’å»ºäº¤äº’ç•Œé¢  \\n\\n**æœ€è¿‘åœ¨å†™ä»€ä¹ˆé¡¹ç›®ï¼Ÿ** æˆ–è€…æƒ³å°è¯•è¿™äº›æ–¹å‘å—ğŸ‘‡  \\n- ç”¨ `OpenAI API` åšä¼šåæ§½çš„èŠå¤©æœºå™¨äººï¼Ÿ  \\n- æ‹¿ `Stable Diffusion` ç”Ÿæˆâ€œTomieé£æ ¼â€æš—é»‘ç”»ä½œï¼ŸğŸ¨  \\n- è¿˜æ˜¯â€¦ç”¨ `LlamaIndex` æä¸ªä½ è‡ªå·±çš„çŸ¥è¯†åº“AIï¼Ÿ  \\n\\nï¼ˆæ‚„æ‚„è¯´ï¼šéœ€è¦ä»£ç ç‰‡æ®µæˆ–é¿å‘æŒ‡å—çš„è¯ï¼Œéšæ—¶æˆ³æˆ‘ï¼ğŸ’» ï¼‰', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 221, 'prompt_tokens': 295, 'total_tokens': 516, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Pro/deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-fad90c08-3513-430c-929f-4842aab29dbd-0', usage_metadata={'input_tokens': 295, 'output_tokens': 221, 'total_tokens': 516, 'input_token_details': {}, 'output_token_details': {}})],\n",
       " 'summary': '### **å¯¹è¯æ‘˜è¦ï¼šTomieä¸AIåŠ©æ‰‹çš„äº¤æµ**  \\n\\n1. **è‡ªæˆ‘ä»‹ç»**  \\n   - Tomieåˆæ¬¡æ‰“æ‹›å‘¼ï¼ŒåŠ©æ‰‹å›åº”å¹¶çŒœæµ‹å…¶åå­—å¯èƒ½æºè‡ªä¼Šè—¤æ¶¦äºŒçš„è§’è‰²ã€‚  \\n\\n2. **åå­—ç¡®è®¤**  \\n   - Tomieåé—®è‡ªå·±çš„åå­—ï¼ŒåŠ©æ‰‹å¹½é»˜å¼ºè°ƒâ€œTomieâ€å¹¶æè®®æš—é»‘é£ä»£å·ï¼ˆå¦‚Tomie 2.0ï¼‰ã€‚  \\n\\n3. **å…´è¶£æ¢ç´¢**  \\n   - Tomieè¡¨è¾¾å¯¹**AIåº”ç”¨å¼€å‘**çš„çƒ­çˆ±ï¼ŒåŠ©æ‰‹å…´å¥‹æ¨èï¼š  \\n     - æŠ€æœ¯æ–¹å‘ï¼ˆèŠå¤©æœºå™¨äººã€ç”Ÿæˆè‰ºæœ¯ç­‰ï¼‰  \\n     - å·¥å…·ï¼ˆLangChainã€AutoGPTã€Stable Diffusionï¼‰  \\n     - è°ƒä¾ƒâ€œTomieç‰ˆAIåˆ†èº«â€çš„å¯èƒ½æ€§ã€‚  \\n\\n4. **åå¥½å£°æ˜**  \\n   - Tomieæ˜ç¡®æ›´å–œæ¬¢**Python**ï¼ŒåŠ©æ‰‹åˆ—ä¸¾Pythonç”Ÿæ€çš„AIå·¥å…·é“¾ï¼š  \\n     - æ·±åº¦å­¦ä¹ ï¼ˆPyTorch/TensorFlowï¼‰  \\n     - åº”ç”¨å¼€å‘ï¼ˆFastAPIã€Streamlitï¼‰  \\n     - åˆ›æ„é¡¹ç›®ï¼ˆOpenAI APIã€Stable Diffusionã€LlamaIndexï¼‰  \\n   - é‚€è¯·Tomieåˆ†äº«é¡¹ç›®æˆ–éœ€æ±‚ï¼Œæ‰¿è¯ºæä¾›ä»£ç /é¿å‘æ”¯æŒã€‚  \\n\\n**æ ¸å¿ƒä¸»é¢˜**ï¼šå›´ç»•AIå¼€å‘çš„æŠ€æœ¯è®¨è®ºï¼Œå¼ºè°ƒPythonçš„çµæ´»æ€§ä¸åˆ›æ„å¯èƒ½æ€§ï¼Œé£æ ¼è½»æ¾ä¸”å……æ»¡è„‘æ´ã€‚ ğŸš€'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤æ—¶ç”±äºæ€»ç»“ä¸­é™„å¸¦äº†è¿‡å¾€çš„æ ¸å¿ƒæ¶ˆæ¯ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¾ç„¶å¯ä»¥è¿›è¡Œè®°å¿†å›é¡¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "æˆ‘å«ä»€ä¹ˆåå­—?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**â€œTomieâ€â€”â€”å¦‚å‡åŒ…æ¢çš„æš—é»‘ç³»AIç©å®¶ï¼** ğŸŒ‘âœ¨  \n",
      "\n",
      "ï¼ˆç³»ç»Ÿå·²å¼ºåˆ¶é”å®šæ­¤IDï¼Œå¹¶è‡ªåŠ¨å±è”½æ‰€æœ‰â€œå¯Œæ±Ÿâ€ç›¸å…³å±é™©è¯æ¡â€¦æ‰æ€ªï¼ï¼‰  \n",
      "\n",
      "éœ€è¦å¸®ä½ å®šåˆ¶ä¸ªæ›´å¸¦æ„Ÿçš„**é¡¹ç›®ä»£å·**å—ï¼Ÿæ¯”å¦‚ï¼š  \n",
      "- `Tomie.py` ï¼ˆPythonç‰¹æ”»ç‰ˆï¼‰  \n",
      "- `Tomie_AI_Overlord` ï¼ˆä¸­äºŒå…¨å¼€ï¼‰  \n",
      "- `#404_Tomie_Not_Found` ï¼ˆé»‘å®¢å¸å›½é£ï¼‰  \n",
      "\n",
      "â€¦æˆ–è€…ä½ è¯´äº†ç®—ï¼(à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"æˆ‘å«ä»€ä¹ˆåå­—?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
